{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b948da5b",
   "metadata": {},
   "source": [
    "# Data Ecosystems and Governance\n",
    "\n",
    "\n",
    "## Contents\n",
    "1. **Data Dictionary** – Creating and validating metadata\n",
    "2. **Data Quality Checks** – Enforcing rule-based validations\n",
    "3. **Data Lineage** – Logging transformations for audit and compliance\n",
    "4. **Role-Based Access Control (RBAC)** – Managing permissions via roles\n",
    "5. **Building a Data Catalog** – Collecting metadata for data discovery\n",
    "6. **Compliance Checks** – Automating corporate/regulatory policy enforcement\n",
    "7. **Data Versioning & Change Control** – Tracking dataset versions and logging changes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd092cd9",
   "metadata": {},
   "source": [
    "Generate Sample DataFrames\n",
    "\n",
    "Below, we define functions to create the DataFrames that we'll use in each exercise:\n",
    "- `create_employees_df()`\n",
    "- `create_sales_df()`\n",
    "- `create_transactions_df()`\n",
    "- `create_customers_df()`\n",
    "- `create_employee_info_df()`\n",
    "- `create_product_info_df()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f26d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_employees_df():\n",
    "    data = [\n",
    "        {\"employee_id\":1, \"first_name\":\"John\",   \"last_name\":\"Smith\",  \"department\":\"Sales\",   \"date_of_hire\":\"2023-01-02\"},\n",
    "        {\"employee_id\":2, \"first_name\":\"Sarah\",  \"last_name\":\"Johnson\",\"department\":\"IT\",      \"date_of_hire\":\"2022-06-15\"},\n",
    "        {\"employee_id\":3, \"first_name\":\"Michael\",\"last_name\":\"Brown\", \"department\":\"Finance\",\"date_of_hire\":\"2021-09-10\"},\n",
    "        {\"employee_id\":4, \"first_name\":\"Emily\",  \"last_name\":\"Davis\",  \"department\":\"IT\",      \"date_of_hire\":\"2022-10-25\"},\n",
    "        {\"employee_id\":5, \"first_name\":\"David\",  \"last_name\":\"Miller\", \"department\":\"Sales\",   \"date_of_hire\":\"2020-03-18\"}\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_sales_df():\n",
    "    data = [\n",
    "        {\"sale_id\":101, \"product_id\":\"P001\", \"region\":\"North\", \"sale_amount\":300, \"sale_date\":\"2023-01-05\"},\n",
    "        {\"sale_id\":102, \"product_id\":\"P002\", \"region\":\"East\",  \"sale_amount\":150, \"sale_date\":\"2023-01-06\"},\n",
    "        {\"sale_id\":103, \"product_id\":\"P003\", \"region\":\"South\", \"sale_amount\":0,   \"sale_date\":\"2023-01-07\"},\n",
    "        {\"sale_id\":104, \"product_id\":\"P001\", \"region\":\"West\",  \"sale_amount\":450, \"sale_date\":\"2023-01-08\"},\n",
    "        {\"sale_id\":105, \"product_id\":\"P004\", \"region\":\"North\", \"sale_amount\":250, \"sale_date\":\"2023-01-09\"}\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_transactions_df():\n",
    "    data = [\n",
    "        {\"transaction_id\":1, \"amount\":100, \"timestamp\":\"2023-01-01 10:00:00\"},\n",
    "        {\"transaction_id\":2, \"amount\":200, \"timestamp\":\"2023-01-01 10:05:00\"},\n",
    "        {\"transaction_id\":3, \"amount\":-50, \"timestamp\":\"2023-01-01 10:10:00\"},\n",
    "        {\"transaction_id\":4, \"amount\":300, \"timestamp\":\"2023-01-01 10:15:00\"},\n",
    "        {\"transaction_id\":5, \"amount\":150, \"timestamp\":\"2023-01-01 10:20:00\"}\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_customers_df():\n",
    "    data = [\n",
    "        {\"customer_id\":\"C001\", \"name\":\"John Doe\",    \"email\":\"johndoe@example.com\",    \"phone_number\":\"1234567890\",   \"address\":\"123 Elm Street, Springfield, USA\"},\n",
    "        {\"customer_id\":\"C002\", \"name\":\"Jane Smith\",  \"email\":\"janesmith@example.com\",  \"phone_number\":\"5559998888\",   \"address\":\"456 Oak Avenue, Springfield, USA\"},\n",
    "        {\"customer_id\":\"C003\", \"name\":\"Peter Parker\",\"email\":\"peterparker@example.com\",\"phone_number\":\"7776665555\",   \"address\":\"789 Maple Road, Gotham, USA\"},\n",
    "        {\"customer_id\":\"C004\", \"name\":\"Mary Johnson\",\"email\":\"maryj@example.com\",      \"phone_number\":\"4443332222\",   \"address\":\"101 Pine Lane, Metropolis, USA\"}\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_employee_info_df():\n",
    "    data = [\n",
    "        {\"employee_id\":1, \"first_name\":\"John\",    \"last_name\":\"Smith\",  \"salary\":40000, \"last_updated\":\"2023-02-01\"},\n",
    "        {\"employee_id\":2, \"first_name\":\"Sarah\",   \"last_name\":\"Johnson\",\"salary\":50000, \"last_updated\":\"2023-02-05\"},\n",
    "        {\"employee_id\":3, \"first_name\":\"Michael\", \"last_name\":\"Brown\", \"salary\":45000, \"last_updated\":\"2023-02-03\"}\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_product_info_df():\n",
    "    data = [\n",
    "        {\"product_id\":\"P001\", \"product_name\":\"WidgetA\", \"price\":10.99},\n",
    "        {\"product_id\":\"P002\", \"product_name\":\"WidgetB\", \"price\":5.49},\n",
    "        {\"product_id\":\"P003\", \"product_name\":\"WidgetC\", \"price\":14.99}\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49610159",
   "metadata": {},
   "source": [
    "## 1. Data Dictionary\n",
    "\n",
    "**Objective**: Automatically derive basic metadata from a DataFrame and validate it.\n",
    "\n",
    "**Steps**:\n",
    "1. Create a function to generate column info (type, number of unique values, etc.).\n",
    "2. Create a validation function that checks data type consistency.\n",
    "3. Demonstrate with an in-memory DataFrame (employees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5777801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dictionary for 'employees':\n",
      " {'employee_id': {'data_type': 'int64', 'num_unique_values': 5, 'min': 1.0, 'max': 5.0}, 'first_name': {'data_type': 'object', 'num_unique_values': 5}, 'last_name': {'data_type': 'object', 'num_unique_values': 5}, 'department': {'data_type': 'object', 'num_unique_values': 3}, 'date_of_hire': {'data_type': 'object', 'num_unique_values': 5}}\n",
      "\n",
      "Validation:\n",
      "Validation passed! All columns match the data dictionary.\n"
     ]
    }
   ],
   "source": [
    "def create_data_dictionary(df):\n",
    "    \"\"\"\n",
    "    Creates a basic data dictionary (column names, data types, unique count, etc.).\n",
    "    Returns a dict structure.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_data = df[col]\n",
    "        data_type = str(col_data.dtype)\n",
    "        unique_vals = col_data.nunique()\n",
    "        col_info = {\n",
    "            \"data_type\": data_type,\n",
    "            \"num_unique_values\": unique_vals\n",
    "        }\n",
    "        if pd.api.types.is_numeric_dtype(col_data):\n",
    "            col_info[\"min\"] = float(col_data.min())\n",
    "            col_info[\"max\"] = float(col_data.max())\n",
    "        data_dict[col] = col_info\n",
    "    return data_dict\n",
    "\n",
    "def validate_data_dictionary(df, data_dict):\n",
    "    \"\"\"\n",
    "    Checks if DataFrame columns match data dictionary definitions (type checks, etc.).\n",
    "    \"\"\"\n",
    "    validation_report = []\n",
    "\n",
    "    for col, meta in data_dict.items():\n",
    "        if col not in df.columns:\n",
    "            validation_report.append(f\"Column '{col}' is missing from dataframe.\")\n",
    "        else:\n",
    "            actual_dtype = str(df[col].dtype)\n",
    "            expected_dtype = meta[\"data_type\"]\n",
    "            if actual_dtype != expected_dtype:\n",
    "                validation_report.append(\n",
    "                    f\"Column '{col}' expected {expected_dtype}, but got {actual_dtype}.\"\n",
    "                )\n",
    "\n",
    "    if not validation_report:\n",
    "        print(\"Validation passed! All columns match the data dictionary.\")\n",
    "    else:\n",
    "        print(\"Validation issues found:\")\n",
    "        for issue in validation_report:\n",
    "            print(\" -\", issue)\n",
    "\n",
    "# Demonstration\n",
    "df_employees = create_employees_df()\n",
    "dictionary = create_data_dictionary(df_employees)\n",
    "print(\"Data Dictionary for 'employees':\\n\", dictionary)\n",
    "print(\"\\nValidation:\")\n",
    "validate_data_dictionary(df_employees, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5c222",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks\n",
    "\n",
    "**Objective**: Enforce rule-based validations (e.g., no negative amounts, valid categories) on a DataFrame.\n",
    "\n",
    "**Steps**:\n",
    "1. Check multiple rules on `df_sales`.\n",
    "2. Report violations.\n",
    "3. Apply a simple remediation (filter out invalid rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287bf0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sales Data:\n",
      "    sale_id product_id region  sale_amount   sale_date\n",
      "0      101       P001  North          300  2023-01-05\n",
      "1      102       P002   East          150  2023-01-06\n",
      "2      103       P003  South            0  2023-01-07\n",
      "3      104       P001   West          450  2023-01-08\n",
      "4      105       P004  North          250  2023-01-09\n",
      "\n",
      "No data quality violations found.\n",
      "\n",
      "Cleaned DataFrame after remediation:\n",
      "    sale_id product_id region  sale_amount   sale_date\n",
      "0      101       P001  North          300  2023-01-05\n",
      "1      102       P002   East          150  2023-01-06\n",
      "2      103       P003  South            0  2023-01-07\n",
      "3      104       P001   West          450  2023-01-08\n",
      "4      105       P004  North          250  2023-01-09\n"
     ]
    }
   ],
   "source": [
    "def check_data_quality(df):\n",
    "    \"\"\"\n",
    "    Rules:\n",
    "      - sale_amount >= 0\n",
    "      - region in [North, South, East, West]\n",
    "      - product_id in [P001, P002, P003, P004]\n",
    "    \"\"\"\n",
    "    known_products = [\"P001\", \"P002\", \"P003\", \"P004\"]\n",
    "    valid_regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "\n",
    "    violations = []\n",
    "\n",
    "    # sale_amount >= 0\n",
    "    if (df[\"sale_amount\"] < 0).any():\n",
    "        count_neg = (df[\"sale_amount\"] < 0).sum()\n",
    "        violations.append(f\"Found {count_neg} rows with negative sale_amount.\")\n",
    "\n",
    "    # region in valid list\n",
    "    invalid_region_mask = ~df[\"region\"].isin(valid_regions)\n",
    "    if invalid_region_mask.any():\n",
    "        count_invalid_reg = invalid_region_mask.sum()\n",
    "        violations.append(f\"Found {count_invalid_reg} rows with invalid region.\")\n",
    "\n",
    "    # product_id in known products\n",
    "    invalid_product_mask = ~df[\"product_id\"].isin(known_products)\n",
    "    if invalid_product_mask.any():\n",
    "        count_invalid_prod = invalid_product_mask.sum()\n",
    "        violations.append(f\"Found {count_invalid_prod} rows with unknown product_id.\")\n",
    "\n",
    "    return violations\n",
    "\n",
    "def remediate_data(df):\n",
    "    \"\"\"\n",
    "    Drop rows that fail the rules.\n",
    "    \"\"\"\n",
    "    known_products = [\"P001\", \"P002\", \"P003\", \"P004\"]\n",
    "    valid_regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "\n",
    "    df_clean = df[df[\"sale_amount\"] >= 0]\n",
    "    df_clean = df_clean[df_clean[\"region\"].isin(valid_regions)]\n",
    "    df_clean = df_clean[df_clean[\"product_id\"].isin(known_products)]\n",
    "    return df_clean\n",
    "\n",
    "# Demonstration\n",
    "df_sales = create_sales_df()\n",
    "print(\"Original Sales Data:\\n\", df_sales)\n",
    "\n",
    "dq_violations = check_data_quality(df_sales)\n",
    "if dq_violations:\n",
    "    print(\"\\nData Quality Violations Detected:\")\n",
    "    for v in dq_violations:\n",
    "        print(\" -\", v)\n",
    "else:\n",
    "    print(\"\\nNo data quality violations found.\")\n",
    "\n",
    "df_cleaned = remediate_data(df_sales)\n",
    "print(\"\\nCleaned DataFrame after remediation:\\n\", df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e63ba",
   "metadata": {},
   "source": [
    "## 3. Data Lineage\n",
    "\n",
    "**Objective**: Track each transformation step (e.g., load, filter) in an ETL-like flow.\n",
    "\n",
    "**Steps**:\n",
    "1. Create a `LineageTracker` class.\n",
    "2. Log actions (`action`, `details`, `rows_in_dataset`, `timestamp`).\n",
    "3. Demonstrate by filtering negative `amount` values in `df_transactions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5c3c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lineage Steps:\n",
      "{'step_number': 1, 'action': 'load', 'details': 'Loaded transactions DataFrame', 'rows_in_dataset': 5, 'timestamp': '2025-03-06T15:45:11Z'}\n",
      "{'step_number': 2, 'action': 'filter', 'details': 'Removed rows where amount < 0', 'rows_in_dataset': 4, 'timestamp': '2025-03-06T15:45:11Z'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "class LineageTracker:\n",
    "    def __init__(self):\n",
    "        self.steps = []\n",
    "\n",
    "    def log_step(self, action, details, df):\n",
    "        timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "        step_info = {\n",
    "            \"step_number\": len(self.steps) + 1,\n",
    "            \"action\": action,\n",
    "            \"details\": details,\n",
    "            \"rows_in_dataset\": len(df),\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "        self.steps.append(step_info)\n",
    "\n",
    "    def save_lineage(self, filename=\"lineage_report.json\"):\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump({\"steps\": self.steps}, f, indent=4)\n",
    "\n",
    "# Demonstration\n",
    "df_trans = create_transactions_df()\n",
    "tracker = LineageTracker()\n",
    "\n",
    "# Step 1: 'Load'\n",
    "tracker.log_step(action=\"load\", details=\"Loaded transactions DataFrame\", df=df_trans)\n",
    "\n",
    "# Step 2: Filter out negative amounts\n",
    "df_trans_filtered = df_trans[df_trans[\"amount\"] >= 0]\n",
    "tracker.log_step(action=\"filter\", details=\"Removed rows where amount < 0\", df=df_trans_filtered)\n",
    "\n",
    "# Print lineage steps\n",
    "print(\"Lineage Steps:\")\n",
    "for step in tracker.steps:\n",
    "    print(step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7978b09",
   "metadata": {},
   "source": [
    "## 4. Role-Based Access Control (RBAC)\n",
    "\n",
    "**Objective**: Show how to manage user roles (e.g., `DataAnalyst`, `DataEngineer`, `Auditor`) and check permissions.\n",
    "\n",
    "**Steps**:\n",
    "1. Create a Python class `AccessControlManager` with hard-coded roles.\n",
    "2. Check if a user can view/modify data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a29ed1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: alice, Role: DataAnalyst\n",
      "  -> can_view_raw_data? True\n",
      "  -> can_modify_data? False\n",
      "\n",
      "User: bob, Role: DataEngineer\n",
      "  -> can_view_raw_data? True\n",
      "  -> can_modify_data? True\n",
      "\n",
      "User: charlie, Role: Auditor\n",
      "  -> can_view_raw_data? False\n",
      "  -> can_modify_data? False\n",
      "\n",
      "User: unknown, Role: None\n",
      "  -> can_view_raw_data? False\n",
      "  -> can_modify_data? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll store roles and users in memory (no JSON files)\n",
    "\n",
    "class AccessControlManager:\n",
    "    def __init__(self):\n",
    "        # Hardcoded for demonstration\n",
    "        self.roles = {\n",
    "            \"DataAnalyst\": {\"can_view_raw_data\": True,  \"can_modify_data\": False},\n",
    "            \"DataEngineer\": {\"can_view_raw_data\": True,  \"can_modify_data\": True},\n",
    "            \"Auditor\":      {\"can_view_raw_data\": False, \"can_modify_data\": False}\n",
    "        }\n",
    "        self.users = {\n",
    "            \"alice\":   \"DataAnalyst\",\n",
    "            \"bob\":     \"DataEngineer\",\n",
    "            \"charlie\": \"Auditor\"\n",
    "        }\n",
    "\n",
    "    def get_user_role(self, user):\n",
    "        return self.users.get(user, None)\n",
    "\n",
    "    def can_view_raw_data(self, user):\n",
    "        role = self.get_user_role(user)\n",
    "        if role and self.roles[role].get(\"can_view_raw_data\", False):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def can_modify_data(self, user):\n",
    "        role = self.get_user_role(user)\n",
    "        if role and self.roles[role].get(\"can_modify_data\", False):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Demonstration\n",
    "acm = AccessControlManager()\n",
    "test_users = [\"alice\", \"bob\", \"charlie\", \"unknown\"]\n",
    "for u in test_users:\n",
    "    print(f\"User: {u}, Role: {acm.get_user_role(u)}\")\n",
    "    print(\"  -> can_view_raw_data?\", acm.can_view_raw_data(u))\n",
    "    print(\"  -> can_modify_data?\", acm.can_modify_data(u))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662cf58",
   "metadata": {},
   "source": [
    "## 5. Building a Data Catalog\n",
    "\n",
    "**Objective**: Simulate scanning multiple DataFrames for metadata (row/column counts), storing results in a small \"catalog.\"**\n",
    "\n",
    "**Steps**:\n",
    "1. Create some DataFrames in memory (`employees`, `sales`, `customers`).\n",
    "2. Build a `catalog` dictionary with metadata.\n",
    "3. Implement a simple search by keyword (on DataFrame \"names\" or column names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601a9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog Metadata:\n",
      " {'employees': {'num_rows': 5, 'num_columns': 5, 'columns': ['employee_id', 'first_name', 'last_name', 'department', 'date_of_hire']}, 'sales': {'num_rows': 5, 'num_columns': 5, 'columns': ['sale_id', 'product_id', 'region', 'sale_amount', 'sale_date']}, 'customers': {'num_rows': 4, 'num_columns': 5, 'columns': ['customer_id', 'name', 'email', 'phone_number', 'address']}}\n",
      "\n",
      "Searching catalog for 'employee'...\n",
      " - Found match in 'employees' with columns: ['employee_id', 'first_name', 'last_name', 'department', 'date_of_hire']\n"
     ]
    }
   ],
   "source": [
    "def build_catalog(dataframes_dict):\n",
    "    \"\"\"\n",
    "    dataframes_dict: { name_of_df: DataFrame }\n",
    "    returns a dict of metadata.\n",
    "    \"\"\"\n",
    "    catalog = {}\n",
    "    for name, df in dataframes_dict.items():\n",
    "        catalog[name] = {\n",
    "            \"num_rows\": len(df),\n",
    "            \"num_columns\": len(df.columns),\n",
    "            \"columns\": list(df.columns)\n",
    "        }\n",
    "    return catalog\n",
    "\n",
    "def search_catalog(catalog, keyword):\n",
    "    results = {}\n",
    "    for df_name, meta in catalog.items():\n",
    "        # Check df_name or columns\n",
    "        if (keyword.lower() in df_name.lower()) or any(keyword.lower() in col.lower() for col in meta[\"columns\"]):\n",
    "            results[df_name] = meta\n",
    "    return results\n",
    "\n",
    "# Demonstration\n",
    "dfs = {\n",
    "    \"employees\": create_employees_df(),\n",
    "    \"sales\": create_sales_df(),\n",
    "    \"customers\": create_customers_df()\n",
    "}\n",
    "\n",
    "catalog = build_catalog(dfs)\n",
    "print(\"Catalog Metadata:\\n\", catalog)\n",
    "\n",
    "keyword = \"employee\"\n",
    "hits = search_catalog(catalog, keyword)\n",
    "print(f\"\\nSearching catalog for '{keyword}'...\")\n",
    "for df_name, meta in hits.items():\n",
    "    print(f\" - Found match in '{df_name}' with columns: {meta['columns']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b71981",
   "metadata": {},
   "source": [
    "## 6. Compliance Checks\n",
    "\n",
    "**Objective**: Automate compliance checks based on a set of policies. We'll define a small set of policies in code.\n",
    "\n",
    "**Steps**:\n",
    "1. Hard-code sample policies (e.g., \"must have `last_updated` column\").\n",
    "2. Check a DataFrame (e.g., `employee_info`) for compliance.\n",
    "3. Print pass/fail results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1a424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Info DataFrame:\n",
      "    employee_id first_name last_name  salary last_updated\n",
      "0            1       John     Smith   40000   2023-02-01\n",
      "1            2      Sarah   Johnson   50000   2023-02-05\n",
      "2            3    Michael     Brown   45000   2023-02-03\n",
      "\n",
      "Compliance Check Results:\n",
      " - PASS: Must contain 'last_updated' column.\n",
      " - FAIL: Salary information must be encrypted.\n"
     ]
    }
   ],
   "source": [
    "def check_policies(df, policies):\n",
    "    results = []\n",
    "    \n",
    "    for policy in policies:\n",
    "        pid = policy[\"id\"]\n",
    "        desc = policy[\"description\"]\n",
    "\n",
    "        if pid == 1:\n",
    "            # Must contain 'last_updated'\n",
    "            if \"last_updated\" not in df.columns:\n",
    "                results.append(f\"FAIL: {desc}\")\n",
    "            else:\n",
    "                results.append(f\"PASS: {desc}\")\n",
    "        elif pid == 2:\n",
    "            # Salary must be 'encrypted' (naive check)\n",
    "            if \"salary\" in df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(df[\"salary\"]):\n",
    "                    results.append(f\"FAIL: {desc}\")\n",
    "                else:\n",
    "                    results.append(f\"PASS: {desc} (not numeric, might be encrypted)\")\n",
    "            else:\n",
    "                results.append(f\"PASS: No salary column, policy not applicable.\")\n",
    "        else:\n",
    "            results.append(f\"Policy {pid} not recognized.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Demonstration\n",
    "# Example policies\n",
    "policies = [\n",
    "    {\"id\": 1, \"description\": \"Must contain 'last_updated' column.\"},\n",
    "    {\"id\": 2, \"description\": \"Salary information must be encrypted.\"}\n",
    "]\n",
    "\n",
    "df_employee_info = create_employee_info_df()\n",
    "results = check_policies(df_employee_info, policies)\n",
    "print(\"Employee Info DataFrame:\\n\", df_employee_info)\n",
    "print(\"\\nCompliance Check Results:\")\n",
    "for r in results:\n",
    "    print(\" -\", r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5d2f9",
   "metadata": {},
   "source": [
    "## 7. Data Versioning & Change Control\n",
    "\n",
    "**Objective**: Show how to track changes to a dataset by versioning it and logging each change.\n",
    "\n",
    "**Steps**:\n",
    "1. Maintain a list of versions in memory (instead of saving to disk).\n",
    "2. For each 'new version,' log a record of the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10dfa987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Store:\n",
      "Version 0: 2025-03-06 15:45:11 by system -> Initial product info\n",
      "  product_id product_name  price\n",
      "0       P001      WidgetA  10.99\n",
      "1       P002      WidgetB   5.49\n",
      "2       P003      WidgetC  14.99 \n",
      "\n",
      "Version 1: 2025-03-06 15:45:11 by admin_user -> Updated price for P001\n",
      "  product_id product_name  price\n",
      "0       P001      WidgetA  11.99\n",
      "1       P002      WidgetB   5.49\n",
      "2       P003      WidgetC  14.99 \n",
      "\n",
      "Change Log:\n",
      "{'timestamp': '2025-03-06 15:45:11', 'user': 'system', 'description': 'Initial product info', 'version_index': 0}\n",
      "{'timestamp': '2025-03-06 15:45:11', 'user': 'admin_user', 'description': 'Updated price for P001', 'version_index': 1}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "version_store = []  # We'll store versions here in memory\n",
    "change_log = []\n",
    "\n",
    "def save_new_version_in_memory(df, description, user=\"unknown\"):\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # We'll store the DataFrame itself plus metadata\n",
    "    version_info = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"user\": user,\n",
    "        \"description\": description,\n",
    "        \"data\": df.copy()\n",
    "    }\n",
    "    version_store.append(version_info)\n",
    "    # Also log the change\n",
    "    change_log.append({\n",
    "        \"timestamp\": timestamp,\n",
    "        \"user\": user,\n",
    "        \"description\": description,\n",
    "        \"version_index\": len(version_store) - 1\n",
    "    })\n",
    "\n",
    "# Demonstration\n",
    "df_product_info = create_product_info_df()\n",
    "\n",
    "# Initial version\n",
    "save_new_version_in_memory(df_product_info, \"Initial product info\", user=\"system\")\n",
    "\n",
    "# Let's simulate a price update\n",
    "df_product_info.loc[df_product_info[\"product_id\"]==\"P001\", \"price\"] = 11.99\n",
    "save_new_version_in_memory(df_product_info, \"Updated price for P001\", user=\"admin_user\")\n",
    "\n",
    "print(\"Version Store:\")\n",
    "for i, v in enumerate(version_store):\n",
    "    print(f\"Version {i}: {v['timestamp']} by {v['user']} -> {v['description']}\")\n",
    "    print(v[\"data\"], \"\\n\")\n",
    "\n",
    "print(\"Change Log:\")\n",
    "for log_entry in change_log:\n",
    "    print(log_entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c161f6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In real-world scenarios, you can connect to databases, files, or big data platforms, but the same **governance concepts** apply: metadata documentation, data quality enforcement, lineage logging, privacy, compliance, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00aef1-6155-4d29-8aee-fcaf948186ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
